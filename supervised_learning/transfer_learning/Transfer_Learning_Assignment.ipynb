{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Sequential\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23591810 (90.00 MB)\n",
      "Trainable params: 4098 (16.01 KB)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2 # specifies the number of classes to be classified\n",
    "\n",
    "# Then add the new models \n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(ResNet50(include_top=False, pooling='avg', weights='imagenet'))\n",
    "new_model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# since first layer is already trained, restrict/freeze its retraining\n",
    "\n",
    "new_model.layers[0].trainable = False\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 512)               14714688  \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14715714 (56.14 MB)\n",
      "Trainable params: 1026 (4.01 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "v16_model = Sequential()\n",
    "#Add the VGG16 model to our new sequential\n",
    "v16_model.add(VGG16(include_top=False, pooling='avg', weights='imagenet'))\n",
    "#add a dense layer to this new model after the convolutions.\n",
    "v16_model.add(Dense(num_classes, activation='softmax'))\n",
    "#freeze the top layers of our model (vgg16 layers)\n",
    "v16_model.layers[0].trainable = False\n",
    "v16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Mobile net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
      "17225924/17225924 [==============================] - 8s 0us/step\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenet_1.00_224 (Functi  (None, 1024)              3228864   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3230914 (12.32 MB)\n",
      "Trainable params: 2050 (8.01 KB)\n",
      "Non-trainable params: 3228864 (12.32 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mobile_net = Sequential()\n",
    "mobile_net.add(MobileNet(include_top=False, pooling='avg', weights='imagenet'))\n",
    "#add a dense layer to this new model after the convolutions.\n",
    "mobile_net.add(Dense(num_classes, activation='softmax'))\n",
    "#freeze the top layers of our model (vgg16 layers)\n",
    "mobile_net.layers[0].trainable = False\n",
    "mobile_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile resnet model model\n",
    "new_model.compile (optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "v16_model.compile (optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "mobile_net.compile (optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and perfom Augmentation using data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 72 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#load the image data\n",
    "\n",
    "#first, declare the required image size \n",
    "image_size = 224\n",
    "# The image data generator is used to augment our dataset to make sure that our model does not see the same image twice. This is especially important given that we have a relatively small dataset, hence our model is prone to overfitting\n",
    "#Entropic capacity of a model. How much capacity is your model allowed to store\n",
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory('../input/rural_and_urban_photos/train', target_size=(image_size, image_size),\n",
    "                                                     batch_size = 12, \n",
    "                                                     class_mode = 'categorical')\n",
    "validation_generator = data_generator.flow_from_directory('../input/rural_and_urban_photos/val', target_size=(image_size, image_size),\n",
    "                                                     batch_size = 20, \n",
    "                                                     class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and adjust the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the Resnet model\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "6/6 [==============================] - 19s 2s/step - loss: 0.5902 - accuracy: 0.7500 - val_loss: 0.0499 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 5s 781ms/step - loss: 0.0468 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 4s 760ms/step - loss: 0.0309 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 4s 759ms/step - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 4s 756ms/step - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 5s 775ms/step - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0381 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 4s 765ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 4s 764ms/step - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0355 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 5s 798ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 4s 765ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
      "Fitting the VGG16 model\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 11s 2s/step - loss: 1.3720 - accuracy: 0.8333 - val_loss: 0.0093 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 10s 2s/step - loss: 0.1139 - accuracy: 0.9722 - val_loss: 0.0109 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 12s 2s/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0104 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 11s 2s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 12s 2s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 12s 2s/step - loss: 8.8475e-04 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 13s 2s/step - loss: 8.3694e-04 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 13s 2s/step - loss: 7.6020e-04 - accuracy: 1.0000 - val_loss: 0.0101 - val_accuracy: 1.0000\n",
      "Fitting the Mobile net model\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 8s 540ms/step - loss: 0.7887 - accuracy: 0.5694 - val_loss: 0.6885 - val_accuracy: 0.7000\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 1s 204ms/step - loss: 0.3414 - accuracy: 0.8472 - val_loss: 0.2891 - val_accuracy: 0.8500\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 1s 218ms/step - loss: 0.1834 - accuracy: 0.9583 - val_loss: 0.2871 - val_accuracy: 0.8500\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 1s 207ms/step - loss: 0.1311 - accuracy: 0.9861 - val_loss: 0.3012 - val_accuracy: 0.8500\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 1s 214ms/step - loss: 0.1060 - accuracy: 0.9861 - val_loss: 0.2417 - val_accuracy: 0.8500\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 1s 210ms/step - loss: 0.0911 - accuracy: 0.9722 - val_loss: 0.2593 - val_accuracy: 0.8500\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 1s 217ms/step - loss: 0.0802 - accuracy: 1.0000 - val_loss: 0.2407 - val_accuracy: 0.8500\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 1s 223ms/step - loss: 0.0712 - accuracy: 1.0000 - val_loss: 0.2348 - val_accuracy: 0.8500\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 1s 223ms/step - loss: 0.0691 - accuracy: 1.0000 - val_loss: 0.2624 - val_accuracy: 0.8500\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 1s 239ms/step - loss: 0.0548 - accuracy: 1.0000 - val_loss: 0.2163 - val_accuracy: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21f3fe725d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model to the data \n",
    "print('Fitting the Resnet model')\n",
    "new_model.fit(train_generator, epochs=10, steps_per_epoch=6, validation_data=validation_generator, validation_steps=1)\n",
    "print('Fitting the VGG16 model')\n",
    "v16_model.fit(train_generator, epochs=10, steps_per_epoch=6, validation_data=validation_generator, validation_steps=1)\n",
    "print('Fitting the Mobile net model')\n",
    "mobile_net.fit(train_generator, epochs=10, steps_per_epoch=6, validation_data=validation_generator, validation_steps=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
